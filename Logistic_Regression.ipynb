{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "rxG5ifZ3vGVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1:  What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Answer 1. Logistic Regression\n",
        "\n",
        "Definition: Logistic Regression is a statistical and machine learning model used for classification problems, where the target variable is categorical (e.g., Yes/No, 0/1, Spam/Not Spam).\n",
        "\n",
        "How it works:\n",
        "\n",
        "Instead of predicting a continuous value, it predicts the probability that an input belongs to a certain class.\n",
        "\n",
        "It uses the sigmoid (logistic) function to squeeze outputs into the range 0 to 1.\n",
        "\n",
        "Decision boundary: If probability > 0.5 → class 1, else class 0.\n",
        "\n",
        "Example: Predicting if a customer will buy a product (Yes/No) based on income and age.\n",
        "\n",
        "\n",
        "2. Linear Regression\n",
        "\n",
        "Definition: Linear Regression is used for regression problems, where the target variable is continuous (e.g., predicting house prices, salary, temperature).\n",
        "\n",
        "How it works:\n",
        "\n",
        "It fits a straight line (or hyperplane) to minimize the error between predicted and actual values.\n",
        "\n",
        "Linear Regression → Predicts numbers\n",
        "\n",
        "Logistic Regression → Predicts categories (via probabilities)\n",
        "\n",
        "\n",
        "# Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "Answer 1. Logistic Regression Goal\n",
        "\n",
        "Logistic Regression is about predicting the probability of an event (e.g., Yes/No, 1/0).\n",
        "\n",
        "Probabilities, however, must always lie between 0 and 1.\n",
        "\n",
        "3. Why It’s Important in Logistic Regression\n",
        "\n",
        "Probability Mapping: Converts linear outputs into probabilities.\n",
        "\n",
        "Decision Making:\n",
        "\n",
        "Non-linearity: Although the input is linear, the sigmoid introduces a non-linear transformation, making it suitable for classification.\n",
        "\n",
        "Interpretability: The output directly tells us the probability of belonging to a class (e.g., 0.8 means 80% chance of \"Yes\").\n",
        "\n",
        "4. Visual Intuition\n",
        "\n",
        "The sigmoid curve is S-shaped:\n",
        "\n",
        "For very negative inputs → probability close to 0\n",
        "\n",
        "For very positive inputs → probability close to 1\n",
        "\n",
        "\n",
        "# Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "Answer\n",
        "1. What is Regularization in Logistic Regression?\n",
        "\n",
        "Regularization is a technique used to prevent overfitting by adding a penalty term to the model’s cost function (loss function).\n",
        "\n",
        "In Logistic Regression, the basic cost function is Log Loss (Cross-Entropy Loss).\n",
        "\n",
        "With regularization, we modify it to penalize very large coefficients (weights).\n",
        "\n",
        "\n",
        "2. Why is Regularization Needed?\n",
        "\n",
        "Problem: Without regularization, Logistic Regression may try to fit the training data too closely (overfitting), especially when:\n",
        "\n",
        "There are many features\n",
        "\n",
        "Features are highly correlated\n",
        "\n",
        "Dataset is small or noisy\n",
        "\n",
        "3. Types of Regularization in Logistic Regression\n",
        "(a) L1 Regularization (Lasso)\n",
        "\n",
        "Adds penalty = sum of absolute values of coefficients.\n",
        "\n",
        "(b) L2 Regularization (Ridge)\n",
        "\n",
        "Adds penalty = sum of squared values of coefficients.\n",
        "\n",
        "(c) Elastic Net (Combination of L1 + L2)\n",
        "\n",
        "Balances both feature selection (L1) and coefficient shrinking (L2).\n",
        "\n",
        "\n",
        "4. Benefits of Regularization\n",
        "\n",
        "Prevents overfitting\n",
        "\n",
        "Improves generalization (better performance on new data)\n",
        "\n",
        "Helps deal with multicollinearity (correlated features)\n",
        "\n",
        "Encourages simpler models\n",
        "\n",
        "\n",
        "# Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "Answer\n",
        "1. Why Do We Need Evaluation Metrics?\n",
        "\n",
        "Just checking accuracy is not always enough.\n",
        "\n",
        "Example: If 95% of patients are healthy and only 5% have a disease → a model that always predicts \"healthy\" will have 95% accuracy but is useless in practice.\n",
        "\n",
        "That’s why we use multiple evaluation metrics.\n",
        "\n",
        "\n",
        "2. Common Evaluation Metrics for Classification\n",
        "(a) Accuracy\n",
        "When to use: Good when classes are balanced.\n",
        "\n",
        "Limitation: Misleading if dataset is imbalanced.\n",
        "\n",
        "(b) Precision\n",
        "\n",
        "Out of all predicted positives, how many are actually positive?\n",
        "\n",
        "High Precision → few false positives.\n",
        "\n",
        "Example: Spam detection (better to be precise so genuine emails aren’t marked as spam).\n",
        "\n",
        "(c) Recall (Sensitivity or True Positive Rate)\n",
        "\n",
        "Out of all actual positives, how many did we correctly predict?\n",
        "High Recall → few false negatives.\n",
        "\n",
        "Example: Disease detection (better to catch as many patients as possible).\n",
        "\n",
        "(d) F1-Score\n",
        "\n",
        "Harmonic mean of Precision and Recall:\n",
        "\n",
        "When to use: Best when you need a balance between Precision and Recall.\n",
        "\n",
        "\n",
        "(e) ROC Curve & AUC (Area Under Curve)\n",
        "\n",
        "ROC Curve: Plots True Positive Rate (Recall) vs False Positive Rate.\n",
        "\n",
        "AUC: Measures overall ability of the model to distinguish between classes.\n",
        "\n",
        "Closer to 1 → Better.\n",
        "\n",
        "Example: In credit card fraud detection, a model with AUC = 0.95 is very good at separating fraud from non-fraud.\n",
        "\n",
        "(f) Confusion Matrix\n",
        "\n",
        "A table that shows TP, FP, TN, FN.\n",
        "\n",
        "Helps you see types of errors made by the model.\n",
        "\n",
        "\n",
        "\n",
        "# Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package) (Include your Python code and output in the code box below.)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 3. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train-Test Split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # Increase iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression Model: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Question 6:  Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "answer\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and Target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with L2 Regularization (Ridge)\n",
        "# penalty='l2' is default, but we explicitly specify it\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions and Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Print coefficients and accuracy\n",
        "print(\"Logistic Regression with L2 Regularization (Ridge)\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(\"Model Coefficients (per feature):\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature:25s}: {coef:.4f}\")\n",
        "\n",
        "print(\"\\nIntercept:\", model.intercept_[0])\n",
        "print(f\"\\nAccuracy on Test Set: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "Answer\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "\n",
        "# 2. Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and Target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Classification Report\n",
        "print(\"Logistic Regression with OvR (One-vs-Rest)\")\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n",
        "# Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "answer\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],          # Regularization strength\n",
        "    'penalty': ['l1', 'l2']                # Type of penalty\n",
        "}\n",
        "\n",
        "# 4. Create Logistic Regression model\n",
        "# Note: l1 penalty requires solver='liblinear'\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=5000, multi_class='ovr')\n",
        "\n",
        "# 5. Apply GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid.best_score_:.4f}\")\n",
        "print(f\"Test Set Accuracy: {grid.score(X_test, y_test):.4f}\")\n",
        "\n",
        "\n",
        "# Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        " (Include your Python code and output in the code box below.)\n",
        "\n",
        " answer\n",
        " import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Model WITHOUT Standardization\n",
        "# -----------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# Model WITH Standardization\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=5000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# Print Results\n",
        "# -----------------------------\n",
        "print(\"Logistic Regression Accuracy Comparison\")\n",
        "print(\"--------------------------------------\")\n",
        "print(f\"Without Scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"With Scaling   : {acc_with_scaling:.4f}\")\n",
        "\n",
        "\n",
        "# Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "\n",
        "answer\n",
        "Scenario\n",
        "\n",
        "Task: Predict which customers will respond to a marketing campaign.\n",
        "\n",
        "Challenge: Only 5% positive responses (imbalanced dataset).\n",
        "\n",
        "Goal: Build a robust Logistic Regression model that handles imbalance and provides actionable insights.\n",
        "\n",
        "1. Data Handling\n",
        "\n",
        "Load & Clean Data: Handle missing values (impute with mean/median for numeric, mode for categorical).\n",
        "\n",
        "Feature Engineering:\n",
        "\n",
        "Encode categorical variables (One-Hot Encoding).\n",
        "\n",
        "Create domain-specific features (e.g., recency, frequency, monetary value for purchases).\n",
        "\n",
        "Train/Test Split: Use stratified split so class ratios remain consistent.\n",
        "\n",
        "2. Feature Scaling\n",
        "\n",
        "Since Logistic Regression is sensitive to feature magnitudes, apply StandardScaler or MinMaxScaler to continuous variables.\n",
        "\n",
        "This ensures coefficients are comparable and optimization converges faster.\n",
        "\n",
        "3. Handling Class Imbalance\n",
        "Options:\n",
        "\n",
        "Resampling Techniques:\n",
        "\n",
        "Oversampling: Use SMOTE (Synthetic Minority Oversampling Technique) to increase positive class samples.\n",
        "\n",
        "Undersampling: Reduce majority class, but risk of information loss.\n",
        "\n",
        "Hybrid: Combine both.\n",
        "\n",
        "Class Weights:\n",
        "\n",
        "Set class_weight='balanced' in Logistic Regression.\n",
        "\n",
        "Automatically adjusts weights inversely proportional to class frequencies.\n",
        "\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV with cross-validation.\n",
        "\n",
        "Tune parameters like:\n",
        "\n",
        "C (regularization strength) → controls overfitting/underfitting.\n",
        "\n",
        "penalty (L1 vs L2).\n",
        "\n",
        "solver (liblinear, saga depending on penalty).\n",
        "\n",
        "Use Stratified K-Fold CV to ensure balanced splits during tuning.\n",
        "\n",
        "5. Evaluation Metrics\n",
        "\n",
        "Since dataset is imbalanced, Accuracy is misleading. Focus on:\n",
        "\n",
        "Precision: Of predicted responders, how many are correct?\n",
        "\n",
        "Recall (Sensitivity): Of all actual responders, how many did we catch?\n",
        "\n",
        "F1-Score: Balance between Precision & Recall.\n",
        "\n",
        "ROC-AUC: Measures overall separation between responders/non-responders.\n",
        "\n",
        "PR-AUC (Precision-Recall AUC): More informative when positives are rare.\n",
        "\n",
        "\n",
        "6. Business Deployment Considerations\n",
        "\n",
        "Use probability thresholds instead of just 0.5.\n",
        "\n",
        "Example: If the model predicts P(Response) > 0.3, target that customer.\n",
        "\n",
        "The threshold can be tuned based on cost of marketing vs expected revenue uplift.\n",
        "\n",
        "Periodically retrain the model as customer behavior changes.\n",
        "\n",
        "Combine Logistic Regression with business rules (e.g., only send offers to high-value customers even if probability is moderate)."
      ],
      "metadata": {
        "id": "R1nlNxdivLZ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEpoCngKukFC",
        "outputId": "a77d5492-6f14-4714-b185-6f8c9fde0efb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of dataset:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns] \n",
            "\n",
            "Accuracy of Logistic Regression Model: 0.9561\n"
          ]
        }
      ],
      "source": [
        "# 5\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 3. Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 4. Train-Test Split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # Increase iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 7. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression Model: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# 2. Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and Target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with L2 Regularization (Ridge)\n",
        "# penalty='l2' is default, but we explicitly specify it\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions and Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Print coefficients and accuracy\n",
        "print(\"Logistic Regression with L2 Regularization (Ridge)\")\n",
        "print(\"-------------------------------------------------\")\n",
        "print(\"Model Coefficients (per feature):\")\n",
        "for feature, coef in zip(X.columns, model.coef_[0]):\n",
        "    print(f\"{feature:25s}: {coef:.4f}\")\n",
        "\n",
        "print(\"\\nIntercept:\", model.intercept_[0])\n",
        "print(f\"\\nAccuracy on Test Set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_gT_-MGzye0",
        "outputId": "9a4d5e31-cc29-4e87-f167-73ae462393fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 Regularization (Ridge)\n",
            "-------------------------------------------------\n",
            "Model Coefficients (per feature):\n",
            "mean radius              : 1.0274\n",
            "mean texture             : 0.2215\n",
            "mean perimeter           : -0.3621\n",
            "mean area                : 0.0255\n",
            "mean smoothness          : -0.1562\n",
            "mean compactness         : -0.2377\n",
            "mean concavity           : -0.5326\n",
            "mean concave points      : -0.2837\n",
            "mean symmetry            : -0.2267\n",
            "mean fractal dimension   : -0.0365\n",
            "radius error             : -0.0971\n",
            "texture error            : 1.3706\n",
            "perimeter error          : -0.1814\n",
            "area error               : -0.0872\n",
            "smoothness error         : -0.0225\n",
            "compactness error        : 0.0474\n",
            "concavity error          : -0.0429\n",
            "concave points error     : -0.0324\n",
            "symmetry error           : -0.0347\n",
            "fractal dimension error  : 0.0116\n",
            "worst radius             : 0.1117\n",
            "worst texture            : -0.5089\n",
            "worst perimeter          : -0.0156\n",
            "worst area               : -0.0169\n",
            "worst smoothness         : -0.3077\n",
            "worst compactness        : -0.7727\n",
            "worst concavity          : -1.4286\n",
            "worst concave points     : -0.5109\n",
            "worst symmetry           : -0.7469\n",
            "worst fractal dimension  : -0.1009\n",
            "\n",
            "Intercept: 28.648713947072245\n",
            "\n",
            "Accuracy on Test Set: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "\n",
        "# 2. Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and Target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# 3. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Classification Report\n",
        "print(\"Logistic Regression with OvR (One-vs-Rest)\")\n",
        "print(\"------------------------------------------\")\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syrP-k3M0Uju",
        "outputId": "4b5f53ac-419b-4ee3-e257-7113c5717add"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with OvR (One-vs-Rest)\n",
            "------------------------------------------\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],          # Regularization strength\n",
        "    'penalty': ['l1', 'l2']                # Type of penalty\n",
        "}\n",
        "\n",
        "# 4. Create Logistic Regression model\n",
        "# Note: l1 penalty requires solver='liblinear'\n",
        "log_reg = LogisticRegression(solver='liblinear', max_iter=5000, multi_class='ovr')\n",
        "\n",
        "# 5. Apply GridSearchCV\n",
        "grid = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid.best_score_:.4f}\")\n",
        "print(f\"Test Set Accuracy: {grid.score(X_test, y_test):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6j1vfCu1D1r",
        "outputId": "e7789aec-dcf3-4461-ccdc-a0b32ffa3b1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9583\n",
            "Test Set Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Model WITHOUT Standardization\n",
        "# -----------------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# Model WITH Standardization\n",
        "# -----------------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression(max_iter=5000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# -----------------------------\n",
        "# Print Results\n",
        "# -----------------------------\n",
        "print(\"Logistic Regression Accuracy Comparison\")\n",
        "print(\"--------------------------------------\")\n",
        "print(f\"Without Scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"With Scaling   : {acc_with_scaling:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq1c5vif1ghp",
        "outputId": "39ddefa4-7e8e-4288-c396-8c9bd38ab097"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy Comparison\n",
            "--------------------------------------\n",
            "Without Scaling: 0.9561\n",
            "With Scaling   : 0.9737\n"
          ]
        }
      ]
    }
  ]
}